{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,\"../\")\n",
    "import data_generator as gen\n",
    "import copy\n",
    "from tqdm import tnrange\n",
    "new = np.newaxis\n",
    "\n",
    "debug = False\n",
    "dprint = print if debug else lambda *args, **kwargs : None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "    \n",
    "    def __init__(self,elem=None, shape=None, axes_names=None, sigma=1e-3):\n",
    "        \n",
    "        # Numeric initialization\n",
    "        if (elem is None) and (shape is not None):\n",
    "            #self.elem = np.random.normal(scale=sigma, size=shape)\n",
    "            self.elem = np.random.random(size=shape) # uniform in [0,1]\n",
    "            fact = np.sqrt(self.elem.flatten().shape[0])\n",
    "            #print(\"fact\")\n",
    "            #print(fact)\n",
    "            self.elem /= fact/np.sqrt(2) # heuristic normalization\n",
    "            #self.elem /= np.abs(self.elem).sum()\n",
    "            #self.elem *= 6\n",
    "        elif elem is not None:\n",
    "            self.elem = elem\n",
    "        else:\n",
    "            raise Exception('You have to provide either the elements of the tensor or its shape')\n",
    "            \n",
    "        # Relevant attributes initialization\n",
    "        self.shape = self.elem.shape\n",
    "        self.rank = len(self.shape)\n",
    "        self.aggregations = {}\n",
    "\n",
    "        if axes_names is not None:\n",
    "            try:\n",
    "                if len(axes_names) == self.rank:\n",
    "                    self.history_axes_names = [np.array(axes_names)]\n",
    "                    self.axes_names = np.array(axes_names)\n",
    "                else:\n",
    "                    raise ValueError(\"\") # this error is handled with the except ValueError below\n",
    "            except TypeError:\n",
    "                print(\"=== Warning ===\\nThe object that describes the indexes names have at least to support the built-in len function.\"\\\n",
    "                          +\"\\naxes_names attribute has not been inizialized.\")\n",
    "                self.axes_names = None\n",
    "            except ValueError:\n",
    "                print(\"=== Warning ===\\nThe number of names should match the rank of the tensor.\"\\\n",
    "                          +\"\\naxes_names attribute has not been inizialized.\")\n",
    "                self.axes_names = None\n",
    "        else:\n",
    "            self.axes_names = None\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def transpose(self, permutation):\n",
    "        # permutation is axes_names in the new order\n",
    "        indexes = self.ax_to_index(permutation)\n",
    "        self.elem = np.transpose(self.elem, indexes)\n",
    "        self.update_members(permutation)\n",
    "        return\n",
    "\n",
    "    def ax_to_index(self, axes):\n",
    "        # handle single and multiple indices separately\n",
    "        if type(axes) == str:\n",
    "            return np.where(self.axes_names == axes)[0][0]\n",
    "        else:\n",
    "            return_axes = []\n",
    "            for ax in axes:\n",
    "                return_axes.append(np.where(self.axes_names == ax)[0][0])\n",
    "            return return_axes\n",
    "\n",
    "    \n",
    "    def aggregate(self, axes_names=None, new_ax_name=None):\n",
    "        \"\"\" \n",
    "        Utilization: ...\n",
    "        \"\"\"\n",
    "        # Sanity checks\n",
    "        if (axes_names is None) and (new_ax_name is not None):\n",
    "            axes_names = self.axes_names # if axes_names is None -> aggregate all axes\n",
    "        elif new_ax_name is None:\n",
    "            raise ValueError(\"You have to provide the name of the new axes\")\n",
    "            \n",
    "        if self.axes_names is None:\n",
    "            raise ValueError(\"This function can be called only if the axes names are defined\")\n",
    "            \n",
    "        for name in axes_names:\n",
    "            assert name in self.axes_names, \"The \" + name + \" axes wasn't found in the tensor\"\n",
    "            \n",
    "        dprint(\"Aggregating...\")\n",
    "\n",
    "\n",
    "        # Convert the axes names to their index positions\n",
    "        indexes = self.ax_to_index(axes_names)\n",
    "        \n",
    "        # Store original shape of the aggregated indexes\n",
    "        axes_sizes = np.array(self.shape)[indexes]\n",
    "        self.aggregations[new_ax_name] = dict(zip(axes_names, axes_sizes))\n",
    "        \n",
    "        # Gather the non contracted indexes\n",
    "        all_indexes = set(range(len(self.elem.shape)))\n",
    "        other_indexes = list(all_indexes.difference(set(indexes)))\n",
    "        other_indexes.sort()\n",
    "\n",
    "        dprint(\"axes_numerical+other_axes: \", indexes+other_indexes)\n",
    "\n",
    "        # Perform actual reshaping\n",
    "        self.elem = np.transpose(self.elem, indexes+other_indexes)        \n",
    "        other_sizes = np.array(self.shape)[other_indexes].tolist()\n",
    "        self.elem = self.elem.reshape([-1]+other_sizes)\n",
    "        \n",
    "        # Update class members\n",
    "        self.update_members(np.concatenate([[new_ax_name], self.axes_names[other_indexes]]))\n",
    "        \n",
    "        return\n",
    "        \n",
    "\n",
    "    def disaggregate(self, ax):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        assert ax in self.axes_names, \"The \" + ax + \" ax wasn't found in the tensor.\"\n",
    "        assert ax in self.aggregations.keys(), \"The \" + ax + \" does not represent an aggregated ax.\"\n",
    "        \n",
    "        original_dict = self.aggregations[ax]\n",
    "        original_names = list(original_dict.keys())\n",
    "        original_shape = list(original_dict.values())\n",
    "        \n",
    "        index = self.ax_to_index(ax)\n",
    "        \n",
    "        # transpose to have the aggregated index at the beginning\n",
    "        permutation = [index] + np.arange(index).tolist() + np.arange(index+1, self.rank).tolist()\n",
    "        self.elem = np.transpose(self.elem, permutation)\n",
    "        self.update_members(self.axes_names[permutation])\n",
    "        \n",
    "        # Disaggregate axis by reshaping the tensor\n",
    "        self.elem = self.elem.reshape(original_shape + list(self.shape[1:]))\n",
    "        self.update_members(np.concatenate([original_names, self.axes_names[1:]]))\n",
    "        \n",
    "        # Remove aggregated index from the memory\n",
    "        self.aggregations.pop(ax)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def update_members(self, axes_names):\n",
    "        self.axes_names = np.array(axes_names)\n",
    "        self.shape = self.elem.shape\n",
    "        self.rank = len(self.shape)\n",
    "        return\n",
    "    \n",
    "    def check_names(self):\n",
    "        print(\"=\"*10+\"axes_names type\"+\"=\"*10)\n",
    "        print(type(self.axes_names))\n",
    "        \n",
    "    def __str__(self):\n",
    "        print(\"=\"*10+\" Tensor description \"+\"=\"*10)\n",
    "        print(\"Tensor shape: \", self.shape)\n",
    "        print(\"Tensor rank: \", self.rank)\n",
    "        print(\"Axes names: \", self.axes_names)\n",
    "        return \"\"\n",
    "    \n",
    "    def __add__(self, o): \n",
    "        \"\"\"\n",
    "        Perform sum of two tensors permuting the axes of the second so that they are alligned.\n",
    "        \"\"\"\n",
    "\n",
    "        # check all names match between two tensors\n",
    "        assert np.all(np.isin(self.axes_names, o.axes_names)), \"Error: axes don't match, cannot sum tensors.\"\n",
    "\n",
    "        o.transpose(self.axes_names)\n",
    "        t3 = self.elem + o.elem\n",
    "        T3 = Tensor(elem = t3, axes_names = self.axes_names)\n",
    "        return T3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor\n",
      "t.axes_names:  ['i' 'j' 'k']\n",
      "t.shape:  (2, 3, 2)\n",
      "t.elem:  [[[0.2457754  0.14082872]\n",
      "  [0.29188539 0.32966337]\n",
      "  [0.05893016 0.37033861]]\n",
      "\n",
      " [[0.37709117 0.39029401]\n",
      "  [0.19082283 0.05949995]\n",
      "  [0.06453183 0.13434983]]]\n",
      "\n",
      "After first aggregation:\n",
      "t.axes_names:  ['l' 'k']\n",
      "t.shape:  (6, 2)\n",
      "t.elem:  [[0.2457754  0.14082872]\n",
      " [0.37709117 0.39029401]\n",
      " [0.29188539 0.32966337]\n",
      " [0.19082283 0.05949995]\n",
      " [0.05893016 0.37033861]\n",
      " [0.06453183 0.13434983]]\n",
      "\n",
      "After second aggregation:\n",
      "t.axes_names:  ['i']\n",
      "t.shape:  (12,)\n",
      "t.elem:  [0.2457754  0.14082872 0.37709117 0.39029401 0.29188539 0.32966337\n",
      " 0.19082283 0.05949995 0.05893016 0.37033861 0.06453183 0.13434983]\n",
      "\n",
      "After first disaggregation:\n",
      "t.axes_names:  ['l' 'k']\n",
      "t.shape:  (6, 2)\n",
      "t.elem:  [[0.2457754  0.14082872]\n",
      " [0.37709117 0.39029401]\n",
      " [0.29188539 0.32966337]\n",
      " [0.19082283 0.05949995]\n",
      " [0.05893016 0.37033861]\n",
      " [0.06453183 0.13434983]]\n",
      "\n",
      "After second disaggregation:\n",
      "t.axes_names:  ['j' 'i' 'k']\n",
      "t.shape:  (3, 2, 2)\n",
      "t.elem:  [[[0.2457754  0.14082872]\n",
      "  [0.37709117 0.39029401]]\n",
      "\n",
      " [[0.29188539 0.32966337]\n",
      "  [0.19082283 0.05949995]]\n",
      "\n",
      " [[0.05893016 0.37033861]\n",
      "  [0.06453183 0.13434983]]]\n"
     ]
    }
   ],
   "source": [
    "t = Tensor(shape=[2,3,2], axes_names=['i','j','k'])\n",
    "print(\"Original tensor\")\n",
    "print('t.axes_names: ', t.axes_names)\n",
    "print('t.shape: ', t.shape)\n",
    "print('t.elem: ', t.elem)\n",
    "t.aggregate(axes_names=['j','i'], new_ax_name='l')\n",
    "print('\\nAfter first aggregation:')\n",
    "print('t.axes_names: ', t.axes_names)\n",
    "print('t.shape: ', t.shape)\n",
    "print('t.elem: ', t.elem)\n",
    "t.aggregate(axes_names=['l','k'], new_ax_name='i')\n",
    "print('\\nAfter second aggregation:')\n",
    "print('t.axes_names: ', t.axes_names)\n",
    "print('t.shape: ', t.shape)\n",
    "print('t.elem: ', t.elem)\n",
    "t.disaggregate('i')\n",
    "print('\\nAfter first disaggregation:')\n",
    "print('t.axes_names: ', t.axes_names)\n",
    "print('t.shape: ', t.shape)\n",
    "print('t.elem: ', t.elem)\n",
    "t.disaggregate('l')\n",
    "print('\\nAfter second disaggregation:')\n",
    "print('t.axes_names: ', t.axes_names)\n",
    "print('t.shape: ', t.shape)\n",
    "print('t.elem: ', t.elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _contract_(T1, T2, contracted_axis1, contracted_axis2, common_axis1=[], common_axis2=[]):\n",
    "    \n",
    "    # use this only if we want to give as input string only\n",
    "    #common_axis1 = T1.ax_to_index(common_axis1)\n",
    "    #common_axis2 = T2.ax_to_index(common_axis2)\n",
    "    #contracted_axis1 = T1.ax_to_index(contracted_axis1)\n",
    "    #contracted_axis2 = T2.ax_to_index(contracted_axis2)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert len(common_axis1) == len(common_axis2), \"number of common axes is different\"\n",
    "    \n",
    "    if type(contracted_axis1) != list:\n",
    "        # assuming contracted_axis1/2 is numeric\n",
    "        assert T1.shape[contracted_axis1] == T2.shape[contracted_axis2], \"dimensions of contracted axes do not match\"\n",
    "        contracted_axis1 = [contracted_axis1]\n",
    "        contracted_axis2 = [contracted_axis2]\n",
    "    \n",
    "    for i in range(len(common_axis1)):\n",
    "        # assuming common_axis1/2 is numeric\n",
    "        assert T1.shape[common_axis1[i]] == T2.shape[common_axis2[i]], \"dimensions of common axes do not match\"\n",
    "        \n",
    "    original_shape1 = np.array(T1.shape)\n",
    "    original_shape2 = np.array(T2.shape)\n",
    "        \n",
    "    def perm(contracted_axis, original_shape, common_axis):\n",
    "        # assuming contracted_axis and common_axis list of integers\n",
    "        # astype is for handle the case in the first array is empty, in which the function cannot infere the type\n",
    "        last_axis = np.concatenate((common_axis, contracted_axis)).astype(\"int64\")         \n",
    "\n",
    "        remaining_axis = np.delete(np.arange(len(original_shape)), last_axis)\n",
    "        permutation = np.concatenate((remaining_axis, last_axis))\n",
    "        return permutation\n",
    "\n",
    "    permutation1 = perm(contracted_axis1, original_shape1, common_axis1)\n",
    "    permutation2 = perm(contracted_axis2, original_shape2, common_axis2)\n",
    "\n",
    "    shape1 = original_shape1[permutation1]\n",
    "    shape2 = original_shape2[permutation2]\n",
    "\n",
    "    # param for match the rank of the two shapes\n",
    "    unique1 = len(shape1)-len(common_axis1)-len(contracted_axis1)\n",
    "    unique2 = len(shape2)-len(common_axis1)-len(contracted_axis1)\n",
    "\n",
    "    new_shape1 = np.concatenate((shape1[:unique1],[1 for i in range(unique2)],shape1[unique1:])).astype(\"int64\")\n",
    "    new_shape2 = np.concatenate(([1 for i in range(unique1)],shape2)).astype(\"int64\")\n",
    "    \n",
    "    T1.transpose(T1.axes_names[permutation1])\n",
    "    T2.transpose(T2.axes_names[permutation2])\n",
    "    #print(T1)\n",
    "    #T1.elem = np.transpose(T1.elem, permutation1)\n",
    "    #T2.elem = np.transpose(T2.elem, permutation2)\n",
    "    #print(T1)\n",
    "\n",
    "    #if T1.axes_names is not None:\n",
    "    #T1.axes_names = T1.axes_names[permutation1]\n",
    "    #T2.axes_names = T2.axes_names[permutation2]\n",
    "    #print(T1)\n",
    "    \n",
    "    T3_axes_names = np.concatenate([T1.axes_names[:unique1], T2.axes_names[:T2.rank-len(contracted_axis2)]])\n",
    "    #else: \n",
    "    #    T3_axes_names = None\n",
    "\n",
    "    T3 = (T1.elem.reshape(new_shape1)*T2.elem.reshape(new_shape2))\n",
    "    if len(contracted_axis1) > 0:\n",
    "        # if len(contracted_axis1) == 0 just to tensor product\n",
    "        T3 = T3.sum(axis=-1)\n",
    "        \n",
    "    T3 = Tensor(elem=T3, axes_names=T3_axes_names)\n",
    "    return T3\n",
    "\n",
    "def contract(T1, T2, contracted_axis1=[], contracted_axis2=[], common_axis1=[], common_axis2=[], contracted=None, common=None):\n",
    "        \n",
    "    if contracted is not None:\n",
    "        contracted_axis1 = contracted\n",
    "        contracted_axis2 = contracted\n",
    " \n",
    "    if common is not None:\n",
    "        common_axis1 = common\n",
    "        common_axis2 = common\n",
    "        \n",
    "    if type(common_axis1) == int:\n",
    "        common_axis1 = [common_axis1]\n",
    "    if type(common_axis2) == int:\n",
    "        common_axis2 = [common_axis2]\n",
    "\n",
    "    if type(contracted_axis1) == str:\n",
    "        #contracted_axis1 = np.where(T1.axes_names == contracted_axis1)[0][0]\n",
    "        contracted_axis1 = T1.ax_to_index(contracted_axis1)\n",
    "\n",
    "    if type(contracted_axis2) == str:\n",
    "        #contracted_axis2 = np.where(T2.axes_names == contracted_axis2)[0][0]\n",
    "        contracted_axis2 = T2.ax_to_index(contracted_axis2)\n",
    "        \n",
    "    temp = []\n",
    "    for key in common_axis1:\n",
    "        if type(key) == str:\n",
    "            temp.append(np.where(T1.axes_names == key)[0][0])\n",
    "        else:\n",
    "            temp.append(key)\n",
    "    common_axis1 = temp\n",
    "    # should work something like common_axis1 = T1.ax_to_index(common_axis1)\n",
    "\n",
    "    temp = []\n",
    "    for key in common_axis2:\n",
    "        if type(key) == str:\n",
    "            temp.append(np.where(T2.axes_names == key)[0][0])\n",
    "        else:\n",
    "            temp.append(key)\n",
    "    common_axis2 = temp\n",
    "    \n",
    "    return _contract_(T1, T2, contracted_axis1, contracted_axis2, common_axis1, common_axis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Tensor description ==========\n",
      "Tensor shape:  (1, 2, 5, 6, 4)\n",
      "Tensor rank:  5\n",
      "Axes names:  ['i' 'j' 'n' 'm' 'l']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random([1,2,3,4])\n",
    "y = np.random.random([3,4,5,6])\n",
    "T1 = Tensor(x, axes_names=['i','j','k','l'])\n",
    "T2 = Tensor(y, axes_names=['k','l','n','m'])\n",
    "T3 = contract(T1,T2, contracted='k', common='l')\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_trace(T, ax1, ax2):\n",
    "    \n",
    "    traced_axes = np.array([ax1,ax2])\n",
    "    traced_indexes = T.ax_to_index(traced_axes)\n",
    "    remaining_axis = np.delete(T.axes_names, traced_indexes)\n",
    "    permutation = np.concatenate((traced_axes, remaining_axis))\n",
    "    T.transpose(permutation)\n",
    "    t = T.elem.trace(axis1=0, axis2=1)\n",
    "    T = Tensor(elem=t, axes_names=remaining_axis)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Tensor description ==========\n",
      "Tensor shape:  (4,)\n",
      "Tensor rank:  1\n",
      "Axes names:  ['j']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = Tensor(shape=[3,4,3], axes_names=['i','j','k'])\n",
    "t1 = partial_trace(t, 'i', 'k')\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_svd(T, threshold=0.999):\n",
    "    \n",
    "    if type(T) != Tensor:\n",
    "        raise TypeError(\"This function only support object from the class Tensor\")\n",
    "\n",
    "    if len(T.shape) != 2:\n",
    "        raise ValueError(\"This function only support a 2D tensors\")\n",
    "\n",
    "    U, S, Vh = np.linalg.svd(copy.deepcopy(T.elem))\n",
    "\n",
    "    # computing adaptive bond dimension\n",
    "    cumulative_variance_explained = np.cumsum(S)/S.sum()\n",
    "    #print(cumulative_variance_explained)\n",
    "    index = np.argmax(cumulative_variance_explained>threshold)\n",
    "    m = T.aggregations['i']['left']\n",
    "    m_new = max(10,min(index,m))\n",
    "    \n",
    "    # cut tensors\n",
    "    Vh = Vh[:m_new,:]\n",
    "    U = U[:,:m_new]\n",
    "    S = S[:m_new]\n",
    "    SVh = Vh*S[:,new]\n",
    "    \n",
    "    # building new tensors\n",
    "    TU = Tensor(elem=U, axes_names=['i','right'])\n",
    "    TSVh = Tensor(elem=SVh, axes_names=['left','j'])\n",
    "    TU.aggregations['i'] = T.aggregations['i']\n",
    "    TSVh.aggregations['j'] = T.aggregations['j']\n",
    "    \n",
    "    # retrieving original dimensions\n",
    "    TU.disaggregate('i')\n",
    "    TSVh.disaggregate('j')\n",
    "    \n",
    "    return TU, TSVh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, N, M, D=2, L=10, sigma=1e-2):\n",
    "        \n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.L = L\n",
    "        \n",
    "        self.As = []\n",
    "        \n",
    "        # sigma as keyword should be removed if uniform initialization is used\n",
    "        self.As.append(Tensor(shape=[L,M,M,D], axes_names=['l','left','right','d0'], sigma=sigma))\n",
    "        for i in range(1,N):\n",
    "            self.As.append(Tensor(shape=[M,M,D], axes_names=['left','right','d'+str(i)], sigma=sigma))\n",
    "        #self.As.append(Tensor(shape=[M,D], axes_names=['left','d'+str(N-1)], sigma=sigma))\n",
    "        \n",
    "        self.l_pos = 0\n",
    "\n",
    "        \n",
    "    def forward(self, X, train=False):\n",
    "        \n",
    "        assert self.N == X.shape[1], \"The 1 dimension of the input data must be the flattened number of pixels\"\n",
    "\n",
    "        # X should be batch_size x 784 x 2\n",
    "        TX = []\n",
    "        for i in range(self.N):\n",
    "            TX.append(Tensor(elem=X[:,i,:], axes_names=['b','d'+str(i)]))\n",
    "                      \n",
    "        # This must be futher investigate, three ways:\n",
    "        #     * numpy vectorize\n",
    "        #     * list comprehension\n",
    "        #     * multithread\n",
    "                      \n",
    "        #A_TX = np.vectorize(contract)(TX, A, contracted='d'+str(i))\n",
    "        A_TX = [contract(self.As[i], TX[i], contracted='d'+str(i)) for i in range(self.N)]\n",
    "        cum_contraction = []\n",
    "        cum_contraction.append(A_TX[-1])\n",
    "        for j in range(1,self.N): # instead of self.N - 1\n",
    "            tmp_cum = copy.deepcopy(cum_contraction[-1])\n",
    "            tmp_cum = contract(A_TX[-(j+1)], tmp_cum, 'right', 'left', common='b')\n",
    "            cum_contraction.append(tmp_cum)\n",
    "\n",
    "        #if train:\n",
    "        self.cum_contraction = cum_contraction[::-1]\n",
    "        self.left_contraction = None\n",
    "        self.TX = TX\n",
    "\n",
    "        out = partial_trace(self.cum_contraction[0], 'right', 'left') # close the circle\n",
    "        return out\n",
    "\n",
    "    def sweep_step(self, f, y, lr, batch_size):\n",
    "        # ID of the node A at which the output of the net is computed\n",
    "        l = self.l_pos\n",
    "        \n",
    "        # (always true)\n",
    "        B = contract(self.As[l], self.As[l+1], \"right\", \"left\")\n",
    "\n",
    "        # (always true)\n",
    "        # computing all elements for delta_B\n",
    "        # Contributions:\n",
    "        # - TX[l]    (always))\n",
    "        # - TX[l+1]    (always))\n",
    "        # - left_contribution    (for l > 0)\n",
    "        # - cum_contraction[-(l+2)]    (for l < N-2)\n",
    "        # - y-f    (always)\n",
    "        \n",
    "        phi = contract(self.TX[l], self.TX[l+1], common=\"b\")\n",
    "        \n",
    "        if l==0:\n",
    "            # tensor product with broadcasting on batch axis\n",
    "            phi = contract(phi, self.cum_contraction[l+2], common = \"b\")\n",
    "        \n",
    "        elif (l > 0) and (l<(self.N-2)):\n",
    "            # compute new term for the left contribute\n",
    "            new_contribution = contract(self.As[l-1], self.TX[l-1], contracted='d'+str(l-1))\n",
    "            #print(\"new_contribution: \\n\", new_contribution)\n",
    "            if l==1:\n",
    "                # define left_contraction (['right','b'])\n",
    "                self.left_contraction = new_contribution\n",
    "            else:\n",
    "                # update left_contraction (['right','b'])\n",
    "                self.left_contraction = contract(self.left_contraction, new_contribution, 'right', 'left', common='b')\n",
    "            #print(\"self.left_contraction: \\n\", self.left_contraction)\n",
    "            #print('self.cum_contraction[l+2]: \\n', self.cum_contraction[l+2])\n",
    "            circle_contraction = contract(self.cum_contraction[l+2], self.left_contraction, 'right', 'left', common='b')\n",
    "            #print(\"circle_contraction: \\n\", circle_contraction)\n",
    "            # tensor product with broadcasting on batch axis\n",
    "            phi = contract(phi, circle_contraction, common = \"b\")\n",
    "            \n",
    "        else:\n",
    "            new_contribution = contract(self.As[l-1], self.TX[l-1], contracted='d'+str(l-1))\n",
    "            \n",
    "            # update left_contraction (['right','b'])\n",
    "            self.left_contraction = contract(self.left_contraction, new_contribution, 'right', 'left', common='b')\n",
    "            \n",
    "            # tensor product with broadcasting on batch axis\n",
    "            phi = contract(phi, self.left_contraction, common = \"b\")\n",
    "            \n",
    "        f.elem = y-f.elem\n",
    "        #print(\"phi: \\n\",phi)\n",
    "        deltaB = contract(f, phi, contracted=\"b\")\n",
    "        deltaB.elem *= (lr/batch_size)\n",
    "        #print(\"Before SVD\")\n",
    "        #print(\"self.As[%d]: \\n\"%(l), self.As[l])\n",
    "        #print(\"self.As[%d]: \\n\"%(l+1), self.As[l+1])\n",
    "        \n",
    "        #print(\"B: \\n\",B)\n",
    "        #print(\"deltaB :\\n\", deltaB)\n",
    "        left_index = deltaB.ax_to_index('left')\n",
    "        right_index = deltaB.ax_to_index('right')\n",
    "        deltaB.axes_names[left_index] = 'right'\n",
    "        deltaB.axes_names[right_index] = 'left'\n",
    "        # update B\n",
    "        B = B + deltaB\n",
    "\n",
    "        # compute new output of the net\n",
    "        out = contract(B, self.TX[l], contracted='d'+str(l))\n",
    "        out = contract(out, self.TX[l+1], contracted='d'+str(l+1), common='b')\n",
    "        if l == 0:\n",
    "            out = contract(out, self.cum_contraction[l+2], 'right', 'left', common = \"b\")\n",
    "        elif (l > 0) and (l<(self.N-2)):\n",
    "            out = contract(self.left_contraction, out, 'right', 'left', common = \"b\")\n",
    "            out = contract(out, self.cum_contraction[l+2], 'right', 'left', common = \"b\")\n",
    "        else:\n",
    "            out = contract(self.left_contraction, out, 'right', 'left', common = \"b\")\n",
    "        \n",
    "        out = partial_trace(out, 'right', 'left') # close the circle\n",
    "        \n",
    "        # reconstruct optimized network tensors\n",
    "        B.aggregate(axes_names=['d'+str(l),'left'], new_ax_name='i')\n",
    "        B.aggregate(axes_names=['d'+str(l+1),'right','l'], new_ax_name='j')\n",
    "        B.transpose(['i','j'])\n",
    "        self.As[l], self.As[l+1] = tensor_svd(B)\n",
    "        #print(\"After SVD\")\n",
    "        #print(\"self.As[%d]: \\n\"%(l), self.As[l])\n",
    "        #print(\"self.As[%d]: \\n\"%(l+1), self.As[l+1])\n",
    "        \n",
    "        # update position of l\n",
    "        self.l_pos += 1\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def sweep(self, X, y, lr):\n",
    "        \n",
    "        batch_size = len(y)\n",
    "        f = self.forward(X, train = True)\n",
    "        y_pred = np.argmax(f.elem, axis=0)\n",
    "        #print(y_pred)\n",
    "        #print(y)\n",
    "        one_hot_y = np.zeros((y.size, self.L))\n",
    "        one_hot_y[np.arange(y.size),y] = 1\n",
    "        y = one_hot_y.T\n",
    "        \n",
    "        \n",
    "        # compute accuracy\n",
    "        \n",
    "        \n",
    "        for i in tnrange(self.N-1):\n",
    "            f = self.sweep_step(f, y, lr, batch_size)\n",
    "            \n",
    "        # svd \n",
    "        B = contract(self.As[-1], self.As[0], \"right\", \"left\")\n",
    "        # reconstruct optimized network tensors\n",
    "        B.aggregate(axes_names=['d'+str(self.l_pos),'left'], new_ax_name='i')\n",
    "        B.aggregate(axes_names=['d0','right','l'], new_ax_name='j')\n",
    "        B.transpose(['i','j'])\n",
    "        self.As[-1], self.As[0] = tensor_svd(B)\n",
    "        #print(\"self.As[-1]: \\n\", self.As[-1])\n",
    "        #print(\"self.As[0]: \\n\", self.As[0])\n",
    "        \n",
    "        self.l_pos = 0\n",
    "        return f\n",
    "    \n",
    "    def train():\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, label) = gen.create_dataset(1000, sigma=0.7)\n",
    "x = data.reshape(1000,25)\n",
    "\n",
    "def psi(x):\n",
    "    x = np.array((np.sin(np.pi*x/2),np.cos(np.pi*x/2)))\n",
    "    return np.transpose(x, [1,2,0])\n",
    "\n",
    "x = psi(x)\n",
    "\n",
    "m = 800\n",
    "x_train = x[:m]\n",
    "x_test =  x[m:]\n",
    "y_train = label[:m]\n",
    "y_test = label[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = Network(N=25, M=20, L=2)#, sigma=0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicola/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:158: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518aa4f6ef994e969a10c2c9e12b44e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 157.42322084137675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b39cca9ebea4e9cb1b60eef9154da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 796.460018627688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c76d85fecf144beaa371e45c9e119a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 796.2925064531028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d283c78394b74ae9a0a8953627c5979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 797.3885291550098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65524c3a03664bf7ab3739d706d27f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 797.6753587567727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66df88feecd42578e07b5354ed74fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 797.7131335435458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbf9408863c479d82eaf88e6b53d935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6 797.7486953794919\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d9d7484ead4666ae76ecfece71866e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7 797.7836915997398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5670d420a7dc4c5d9ee2cec68542aa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 797.818165961995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd83837c99af463181bd276e018e910f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9 797.8521245106426\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    f = net.sweep(x_train, y_train, lr=1)\n",
    "    print(i, np.abs(f.elem).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, x, y):\n",
    "    f = net.forward(x)\n",
    "    print(f.elem.shape)\n",
    "    y_pred = np.argmax(f.elem, axis=0)\n",
    "    delta_y = np.abs(y_pred-y)\n",
    "    errors = delta_y.sum()\n",
    "    accuracy = (len(y_pred)-errors)/len(y_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Tensor description ==========\n",
      "Tensor shape:  (200, 2)\n",
      "Tensor rank:  2\n",
      "Axes names:  ['b' 'd0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(net.TX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 25, 2)\n",
      "(800,)\n",
      "(2, 800)\n",
      "train_acc:  0.9775\n",
      "(2, 200)\n",
      "test_acc:  0.975\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "train_acc = accuracy(net, x_train, y_train)\n",
    "print('train_acc: ', train_acc)\n",
    "test_acc = accuracy(net, x_test, y_test)\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-07ddec2f9b25>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-07ddec2f9b25>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"def aggregate(self, axes_numerical=None, axes_names=None, new_ax_name=None):\n",
    "            \"\"\" \n",
    "            #Utilization: ...\n",
    "            \"\"\"\n",
    "            dprint = print if debug else lambda *args, **kwargs : None\n",
    "            dprint(\"Aggregating...\")\n",
    "\n",
    "            if (axes_names is None) and (axes_numerical is None):\n",
    "                self.elem = self.elem.reshape(-1)\n",
    "                self.shape = self.elem.shape\n",
    "                self.history_axes_names.append(np.array([new_ax_name]))\n",
    "                self.axes_names = np.array([new_ax_name])\n",
    "                return\n",
    "\n",
    "            if axes_names is not None:\n",
    "                if axes_numerical is None:\n",
    "                    axes_numerical = []\n",
    "                    for name in axes_names:\n",
    "                        if type(name) == str:\n",
    "                            if self.axes_names is not None:\n",
    "                                dprint('name: ', name)\n",
    "                                dprint('self.axes_names: ', self.axes_names)\n",
    "                                dprint(np.where(self.axes_names == name))\n",
    "                                idx = np.where(self.axes_names == name)[0][0]\n",
    "                                axes_numerical.append(idx)\n",
    "                            else:\n",
    "                                raise ValueError(\"=== Error ===\\nThe function was called in the index mode but the tensor wasn't initializated with index names. \")\n",
    "\n",
    "                    #self.axes_names = np.array([new_ax_name] + list(set(self.axes_names).difference(set(axes_names))))\n",
    "                    #self.history_axes_names.append(self.axes_names) \n",
    "                else:\n",
    "                    raise Exeption(\"=== Error ===\\nProvide just one input between axes_numerical and axes_names.\")\n",
    "\n",
    "\n",
    "            if (new_ax_name is not None) and (self.axes_names is not None):\n",
    "\n",
    "                # test this case with new_ax_names and axes_numerical and not axes_names\n",
    "                if axes_names is None:\n",
    "                    axes_names = self.axes_names[axes_numerical]\n",
    "\n",
    "                self.axes_names = np.array([new_ax_name] + list(set(self.axes_names).difference(set(axes_names))))\n",
    "                self.history_axes_names.append(self.axes_names) \n",
    "\n",
    "            all_axes = set(range(len(self.elem.shape)))\n",
    "            #!!!!!!!!!!!!!!!! check without list AND sort\n",
    "            other_axes = list(all_axes.difference(set(axes_numerical)))\n",
    "            other_axes.sort()\n",
    "\n",
    "            dprint(\"axes_numerical+other_axes: \", axes_numerical+other_axes)\n",
    "            self.elem = np.transpose(self.elem, axes_numerical+other_axes)\n",
    "            self.transposed_shape.append(self.elem.shape)\n",
    "\n",
    "            self.permutation_history.append(axes_numerical+other_axes)\n",
    "            self.elem = self.elem.reshape([-1]+list(self.transposed_shape[-1][len(axes_numerical):]))\n",
    "            self.shape = self.elem.shape\n",
    "\n",
    "            return\n",
    "\n",
    "        def disaggregate(self, index):\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                if len(self.permutation_history) > 0:\n",
    "                    while len(self.permutation_history) > 0:\n",
    "                        # get and invert last permutation of shape\n",
    "                        last_permutation = list(self.permutation_history[-1])\n",
    "                        last_rank = len(last_permutation)\n",
    "                        permutation_matrix = np.zeros([last_rank,last_rank])\n",
    "                        permutation_matrix[range(last_rank),last_permutation] = 1\n",
    "                        # disaggregate using last transposed shape\n",
    "                        self.elem = self.elem.reshape(self.transposed_shape[-1])\n",
    "                        # IDEA: original_shape = np.dot(permutation_matrix.T, self.transposed_shape)\n",
    "                        inverse_permutation = np.where(permutation_matrix.T == 1)[1]\n",
    "                        self.elem = np.transpose(self.elem, inverse_permutation)\n",
    "                        # remove history of last aggregation\n",
    "                        self.permutation_history.pop()\n",
    "                        self.transposed_shape.pop()\n",
    "\n",
    "                        if len(self.history_axes_names) > 1:\n",
    "                            self.history_axes_names.pop()\n",
    "                            self.axes_names = self.history_axes_names[-1]\n",
    "\n",
    "                        self.shape = self.elem.shape\n",
    "                    return\n",
    "            except:\n",
    "                print(\"Cannot disaggregate tensor if it was not aggregated in the first place.\")\n",
    "                return\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, N, M, D=2, L=10, sigma=1e-2):\n",
    "        \n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.L = L\n",
    "        \n",
    "        self.As = []\n",
    "        \n",
    "        # sigma as keyword should be removed if uniform initialization is used\n",
    "        self.As.append(Tensor(shape=[L,M,D], axes_names=['l','right','d0'], sigma=sigma))\n",
    "        for i in range(1,N-1):\n",
    "            self.As.append(Tensor(shape=[M,M,D], axes_names=['left','right','d'+str(i)], sigma=sigma))\n",
    "        self.As.append(Tensor(shape=[M,D], axes_names=['left','d'+str(N-1)], sigma=sigma))\n",
    "        \n",
    "        self.l_pos = 0\n",
    "\n",
    "        \n",
    "    def forward(self, X, train=False):\n",
    "        \n",
    "        assert self.N == X.shape[1], \"The 1 dimension of the input data must be the flattened number of pixels\"\n",
    "\n",
    "        # X should be batch_size x 784 x 2\n",
    "        TX = []\n",
    "        for i in range(self.N):\n",
    "            TX.append(Tensor(elem=X[:,i,:], axes_names=['b','d'+str(i)]))\n",
    "                      \n",
    "        # This must be futher investigate, three ways:\n",
    "        #     * numpy vectorize\n",
    "        #     * list comprehension\n",
    "        #     * multithread\n",
    "                      \n",
    "        #A_TX = np.vectorize(contract)(TX, A, contracted='d'+str(i))\n",
    "        A_TX = [contract(self.As[i], TX[i], contracted='d'+str(i)) for i in range(self.N)]\n",
    "        cum_contraction = []\n",
    "        cum_contraction.append(A_TX[-1])\n",
    "        for j in range(self.N-1):\n",
    "            tmp_cum = copy.deepcopy(cum_contraction[-1])\n",
    "            tmp_cum = contract(A_TX[-(j+2)], tmp_cum, 'right', 'left', common='b')\n",
    "            cum_contraction.append(tmp_cum)\n",
    "\n",
    "        if train:\n",
    "            self.cum_contraction = cum_contraction\n",
    "            self.left_contraction = None\n",
    "            self.TX = TX\n",
    "\n",
    "        return cum_contraction[-1]\n",
    "\n",
    "    def sweep_step(self, f, y, lr):\n",
    "        # ID of the node A at which the output of the net is computed\n",
    "        l = self.l_pos\n",
    "        \n",
    "        # (always true)\n",
    "        B = contract(self.As[l], self.As[l+1], \"right\", \"left\")\n",
    "\n",
    "        # (always true)\n",
    "        # computing all elements for delta_B\n",
    "        # Contributions:\n",
    "        # - TX[l]    (always))\n",
    "        # - TX[l+1]    (always))\n",
    "        # - left_contribution    (for l > 0)\n",
    "        # - cum_contraction[-(l+2)]    (for l < N-2)\n",
    "        # - y-f    (always)\n",
    "        \n",
    "        phi = contract(self.TX[l], self.TX[l+1], common=\"b\")\n",
    "        \n",
    "        # compute new term for the left contribute\n",
    "        if l > 0:\n",
    "            new_contribution = contract(self.As[l-1], self.TX[l-1], contracted='d'+str(l-1))\n",
    "            if l==1:\n",
    "                # define left_contraction (['right','b'])\n",
    "                self.left_contraction = new_contribution\n",
    "            else:\n",
    "                # update left_contraction (['right','b'])\n",
    "                self.left_contraction = contract(self.left_contraction, new_contribution, 'right', 'left', common='b')\n",
    "\n",
    "            # tensor product performed if no axes is contracted\n",
    "            phi = contract(phi, self.left_contraction, common='b')\n",
    "            \n",
    "        if l<(self.N-2):\n",
    "            # for all but the last optimization step there is a contribution from the right part\n",
    "            phi = contract(phi, self.cum_contraction[-(l+2)], common = \"b\")\n",
    "        \n",
    "        f.elem = y-f.elem\n",
    "        deltaB = contract(f, phi, contracted=\"b\")\n",
    "        deltaB.elem *= lr\n",
    "\n",
    "        # define tensor_sum for this\n",
    "        B = tensor_sum(B, delta_B)\n",
    "        \n",
    "        # this is bad + in general we will have 2 aggregation steps\n",
    "        B.aggregate(axes_names=['d1','right','l'], new_ax_name='j')\n",
    "        \n",
    "        # d0 -> something else ('i')\n",
    "        B.transpose(['d0','j'])\n",
    "        print(B)\n",
    "        # self.As[l], self.As[l+1] = tensor_svd(B)\n",
    "        \n",
    "        # we could just optimize B and return f\n",
    "        return B #, f\n",
    "    \n",
    "    def sweep(self, X, y, lr):\n",
    "        #g = self.forward(np.ones(X.shape), train = True)\n",
    "        #print(\"=\"*100)\n",
    "        #for A in self.cum_contraction:\n",
    "        #    print(A.elem.sum())\n",
    "        #print(\"=\"*100)\n",
    "        #print(g.elem)\n",
    "        f = self.forward(X, train = True)\n",
    "        one_hot_y = np.zeros((y.size, self.L))\n",
    "        one_hot_y[np.arange(y.size),y] = 1\n",
    "        y = one_hot_y.T\n",
    "        B = self.sweep_step(f, y, lr)\n",
    "        return B\n",
    "    \n",
    "    def train():\n",
    "        return\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return\n",
    "\n",
    "f(*par)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "366.85px",
    "left": "1027px",
    "right": "20px",
    "top": "130px",
    "width": "357.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
